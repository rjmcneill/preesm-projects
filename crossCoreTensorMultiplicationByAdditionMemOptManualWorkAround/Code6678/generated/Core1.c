/** 
 * @file Core1.c
 * @generated by C6678CPrinter
 * @date Sun Apr 19 17:17:02 BST 2015
 */
 

#include "cores.h"
#include "utils.h"
#include "communication.h"
#include "fifo.h"
#include "cache.h"

// Core Global Declaration
extern char *const explode_generateTensors_arra__7;  // explode_generateTensors_arrayA > splitTensorA_6 size:= 1048576*char defined in Core0
extern char *const explode_generateTensors_arra__5;  // explode_generateTensors_arrayB > transpose_7 size:= 1048576*char defined in Core0
extern char *const splitTensorA_0__multiply1_0__0;  // splitTensorA_0 > multiply1_0 size:= 131072*char defined in Core0
extern char *const splitTensorA_1__multiply1_1__0;  // splitTensorA_1 > multiply1_1 size:= 131072*char defined in Core0
extern char *const splitTensorA_2__multiply1_2__0;  // splitTensorA_2 > multiply1_2 size:= 131072*char defined in Core0
extern char *const splitTensorA_3__multiply1_3__0;  // splitTensorA_3 > multiply1_3 size:= 131072*char defined in Core0
extern char *const splitTensorA_4__multiply1_4__0;  // splitTensorA_4 > multiply1_4 size:= 131072*char defined in Core0
extern char *const splitTensorA_5__multiply1_5__0;  // splitTensorA_5 > multiply1_5 size:= 131072*char defined in Core0
extern int *const arrayA_1572864__input__0;  // explode_generateTensors_arrayA_arrayA_1572864 > splitTensorA_6_input size:= 262144*int defined in Core0
extern int *const output0__inputA__0;  // splitTensorA_6_output0 > multiply0_6_inputA size:= 32768*int defined in Core0
extern int *const output1__inputA__3;  // splitTensorA_6_output1 > multiply1_6_inputA size:= 32768*int defined in Core0
extern int *const output2__inputA__7;  // splitTensorA_6_output2 > multiply2_6_inputA size:= 32768*int defined in Core0
extern int *const output3__inputA__2;  // splitTensorA_6_output3 > multiply3_6_inputA size:= 32768*int defined in Core0
extern int *const output4__inputA__4;  // splitTensorA_6_output4 > multiply4_6_inputA size:= 32768*int defined in Core0
extern int *const output5__inputA__2;  // splitTensorA_6_output5 > multiply5_6_inputA size:= 32768*int defined in Core0
extern int *const output6__inputA__5;  // splitTensorA_6_output6 > multiply6_6_inputA size:= 32768*int defined in Core0
extern int *const output7__inputA__5;  // splitTensorA_6_output7 > multiply7_6_inputA size:= 32768*int defined in Core0
extern char *const splitTensorA_6__multiply7_6__0;  // splitTensorA_6 > multiply7_6 size:= 131072*char defined in Core0
extern char *const splitTensorA_6__multiply6_6__0;  // splitTensorA_6 > multiply6_6 size:= 131072*char defined in Core0
extern char *const splitTensorA_6__multiply5_6__0;  // splitTensorA_6 > multiply5_6 size:= 131072*char defined in Core0
extern char *const splitTensorA_6__multiply4_6__0;  // splitTensorA_6 > multiply4_6 size:= 131072*char defined in Core0
extern char *const splitTensorA_6__multiply3_6__0;  // splitTensorA_6 > multiply3_6 size:= 131072*char defined in Core0
extern char *const splitTensorA_6__multiply2_6__0;  // splitTensorA_6 > multiply2_6 size:= 131072*char defined in Core0
extern char *const splitTensorA_6__multiply0_6__0;  // splitTensorA_6 > multiply0_6 size:= 131072*char defined in Core0
extern char *const splitTensorA_7__multiply1_7__0;  // splitTensorA_7 > multiply1_7 size:= 131072*char defined in Core0
extern int *const arrayB_1835008__input__0;  // explode_generateTensors_arrayB_arrayB_1835008 > transpose_7_input size:= 262144*int defined in Core0
extern int *const output__input__2;  // transpose_7_output > splitTensorB_7_input size:= 262144*int defined in Core0
extern char *const splitTensorB_0__multiply1_0__0;  // splitTensorB_0 > multiply1_0 size:= 131072*char defined in Core0
extern char *const splitTensorB_1__multiply1_1__0;  // splitTensorB_1 > multiply1_1 size:= 131072*char defined in Core0
extern char *const splitTensorB_2__multiply1_2__0;  // splitTensorB_2 > multiply1_2 size:= 131072*char defined in Core0
extern char *const splitTensorB_3__multiply1_3__0;  // splitTensorB_3 > multiply1_3 size:= 131072*char defined in Core0
extern char *const splitTensorB_4__multiply1_4__0;  // splitTensorB_4 > multiply1_4 size:= 131072*char defined in Core0
extern char *const splitTensorB_5__multiply1_5__0;  // splitTensorB_5 > multiply1_5 size:= 131072*char defined in Core0
extern char *const splitTensorB_6__multiply1_6__0;  // splitTensorB_6 > multiply1_6 size:= 131072*char defined in Core0
extern int *const output0__inputB__4;  // splitTensorB_7_output0 > multiply0_7_inputB size:= 32768*int defined in Core0
extern int *const output1__inputB__3;  // splitTensorB_7_output1 > multiply1_7_inputB size:= 32768*int defined in Core0
extern int *const output2__inputB__7;  // splitTensorB_7_output2 > multiply2_7_inputB size:= 32768*int defined in Core0
extern int *const output3__inputB__3;  // splitTensorB_7_output3 > multiply3_7_inputB size:= 32768*int defined in Core0
extern int *const output4__inputB__5;  // splitTensorB_7_output4 > multiply4_7_inputB size:= 32768*int defined in Core0
extern int *const output5__inputB__2;  // splitTensorB_7_output5 > multiply5_7_inputB size:= 32768*int defined in Core0
extern int *const output6__inputB__4;  // splitTensorB_7_output6 > multiply6_7_inputB size:= 32768*int defined in Core0
extern int *const output7__inputB__0;  // splitTensorB_7_output7 > multiply7_7_inputB size:= 32768*int defined in Core0
extern char *const splitTensorB_7__multiply7_7__0;  // splitTensorB_7 > multiply7_7 size:= 131072*char defined in Core0
extern char *const splitTensorB_7__multiply6_7__0;  // splitTensorB_7 > multiply6_7 size:= 131072*char defined in Core0
extern char *const splitTensorB_7__multiply5_7__0;  // splitTensorB_7 > multiply5_7 size:= 131072*char defined in Core0
extern char *const splitTensorB_7__multiply4_7__0;  // splitTensorB_7 > multiply4_7 size:= 131072*char defined in Core0
extern char *const splitTensorB_7__multiply3_7__0;  // splitTensorB_7 > multiply3_7 size:= 131072*char defined in Core0
extern char *const splitTensorB_7__multiply2_7__0;  // splitTensorB_7 > multiply2_7 size:= 131072*char defined in Core0
extern char *const splitTensorB_7__multiply0_7__0;  // splitTensorB_7 > multiply0_7 size:= 131072*char defined in Core0
extern char *const multiply0_6__sum_6__0;  // multiply0_6 > sum_6 size:= 1048576*char defined in Core0
extern int *const output1__inputA__2;  // splitTensorA_0_output1 > multiply1_0_inputA size:= 32768*int defined in Core0
extern int *const output1__inputB__2;  // splitTensorB_0_output1 > multiply1_0_inputB size:= 32768*int defined in Core0
extern long *const output__input1__6;  // multiply1_0_output > sum_0_input1 size:= 262144*long defined in Core0
extern char *const multiply1_0__sum_0__0;  // multiply1_0 > sum_0 size:= 1048576*char defined in Core0
extern int *const output1__inputA__7;  // splitTensorA_1_output1 > multiply1_1_inputA size:= 32768*int defined in Core0
extern int *const output1__inputB__6;  // splitTensorB_1_output1 > multiply1_1_inputB size:= 32768*int defined in Core0
extern long *const output__input1__1;  // multiply1_1_output > sum_1_input1 size:= 262144*long defined in Core0
extern char *const multiply1_1__sum_1__0;  // multiply1_1 > sum_1 size:= 1048576*char defined in Core0
extern int *const output1__inputA__5;  // splitTensorA_2_output1 > multiply1_2_inputA size:= 32768*int defined in Core0
extern int *const output1__inputB__5;  // splitTensorB_2_output1 > multiply1_2_inputB size:= 32768*int defined in Core0
extern long *const output__input1__7;  // multiply1_2_output > sum_2_input1 size:= 262144*long defined in Core0
extern char *const multiply1_2__sum_2__0;  // multiply1_2 > sum_2 size:= 1048576*char defined in Core0
extern int *const output1__inputA__0;  // splitTensorA_3_output1 > multiply1_3_inputA size:= 32768*int defined in Core0
extern int *const output1__inputB__1;  // splitTensorB_3_output1 > multiply1_3_inputB size:= 32768*int defined in Core0
extern long *const output__input1__4;  // multiply1_3_output > sum_3_input1 size:= 262144*long defined in Core0
extern char *const multiply1_3__sum_3__0;  // multiply1_3 > sum_3 size:= 1048576*char defined in Core0
extern int *const output1__inputA__6;  // splitTensorA_4_output1 > multiply1_4_inputA size:= 32768*int defined in Core0
extern int *const output1__inputB__0;  // splitTensorB_4_output1 > multiply1_4_inputB size:= 32768*int defined in Core0
extern long *const output__input1__2;  // multiply1_4_output > sum_4_input1 size:= 262144*long defined in Core0
extern char *const multiply1_4__sum_4__0;  // multiply1_4 > sum_4 size:= 1048576*char defined in Core0
extern int *const output1__inputA__1;  // splitTensorA_5_output1 > multiply1_5_inputA size:= 32768*int defined in Core0
extern int *const output1__inputB__4;  // splitTensorB_5_output1 > multiply1_5_inputB size:= 32768*int defined in Core0
extern long *const output__input1__3;  // multiply1_5_output > sum_5_input1 size:= 262144*long defined in Core0
extern char *const multiply1_5__sum_5__0;  // multiply1_5 > sum_5 size:= 1048576*char defined in Core0
extern int *const output1__inputB__7;  // splitTensorB_6_output1 > multiply1_6_inputB size:= 32768*int defined in Core0
extern long *const output__input1__5;  // multiply1_6_output > sum_6_input1 size:= 262144*long defined in Core0
extern int *const output1__inputA__4;  // splitTensorA_7_output1 > multiply1_7_inputA size:= 32768*int defined in Core0
extern long *const output__input1__0;  // multiply1_7_output > sum_7_input1 size:= 262144*long defined in Core0
extern char *const multiply1_7__sum_7__0;  // multiply1_7 > sum_7 size:= 1048576*char defined in Core0
extern char *const multiply2_6__sum_6__0;  // multiply2_6 > sum_6 size:= 1048576*char defined in Core0
extern char *const multiply3_6__sum_6__0;  // multiply3_6 > sum_6 size:= 1048576*char defined in Core0
extern char *const multiply4_6__sum_6__0;  // multiply4_6 > sum_6 size:= 1048576*char defined in Core0
extern char *const multiply5_6__sum_6__0;  // multiply5_6 > sum_6 size:= 1048576*char defined in Core0
extern char *const multiply6_6__sum_6__0;  // multiply6_6 > sum_6 size:= 1048576*char defined in Core0
extern char *const multiply7_6__sum_6__0;  // multiply7_6 > sum_6 size:= 1048576*char defined in Core0
extern long *const output__input0__4;  // multiply0_6_output > sum_6_input0 size:= 262144*long defined in Core0
extern long *const output__input2__3;  // multiply2_6_output > sum_6_input2 size:= 262144*long defined in Core0
extern long *const output__input3__4;  // multiply3_6_output > sum_6_input3 size:= 262144*long defined in Core0
extern long *const output__input4__3;  // multiply4_6_output > sum_6_input4 size:= 262144*long defined in Core0
extern long *const output__input5__1;  // multiply5_6_output > sum_6_input5 size:= 262144*long defined in Core0
extern long *const output__input6__4;  // multiply6_6_output > sum_6_input6 size:= 262144*long defined in Core0
extern long *const output__input7__4;  // multiply7_6_output > sum_6_input7 size:= 262144*long defined in Core0
extern long *const output__arrayC_1572864__0;  // sum_6_output > implode_displayResult_arrayC_arrayC_1572864 size:= 262144*long defined in Core0
extern char *const sum_6__implode_displayResult__0;  // sum_6 > implode_displayResult_arrayC size:= 1048576*char defined in Core0

// Core Global Definitions

void core1(void){
	// Initialisation(s)
	communicationInit();

	// Begin the execution loop 
	while(1){
		busy_barrier();
		receiveStart(); // Core0 > Core1: explode_generateTensors_arra__7 
		receiveEnd(0); // Core0 > Core1: explode_generateTensors_arra__7 
		cache_inv(explode_generateTensors_arra__7, 1048576*sizeof(char));
		receiveStart(); // Core7 > Core1: explode_generateTensors_arra__5 
		receiveEnd(7); // Core7 > Core1: explode_generateTensors_arra__5 
		cache_inv(explode_generateTensors_arra__5, 1048576*sizeof(char));
		receiveStart(); // Core4 > Core1: splitTensorA_0__multiply1_0__0 
		receiveEnd(4); // Core4 > Core1: splitTensorA_0__multiply1_0__0 
		cache_inv(splitTensorA_0__multiply1_0__0, 131072*sizeof(char));
		receiveStart(); // Core2 > Core1: splitTensorA_1__multiply1_1__0 
		receiveEnd(2); // Core2 > Core1: splitTensorA_1__multiply1_1__0 
		cache_inv(splitTensorA_1__multiply1_1__0, 131072*sizeof(char));
		receiveStart(); // Core6 > Core1: splitTensorA_2__multiply1_2__0 
		receiveEnd(6); // Core6 > Core1: splitTensorA_2__multiply1_2__0 
		cache_inv(splitTensorA_2__multiply1_2__0, 131072*sizeof(char));
		receiveStart(); // Core5 > Core1: splitTensorA_3__multiply1_3__0 
		receiveEnd(5); // Core5 > Core1: splitTensorA_3__multiply1_3__0 
		cache_inv(splitTensorA_3__multiply1_3__0, 131072*sizeof(char));
		receiveStart(); // Core0 > Core1: splitTensorA_4__multiply1_4__0 
		receiveEnd(0); // Core0 > Core1: splitTensorA_4__multiply1_4__0 
		cache_inv(splitTensorA_4__multiply1_4__0, 131072*sizeof(char));
		receiveStart(); // Core6 > Core1: splitTensorA_5__multiply1_5__0 
		receiveEnd(6); // Core6 > Core1: splitTensorA_5__multiply1_5__0 
		cache_inv(splitTensorA_5__multiply1_5__0, 131072*sizeof(char));
		splitA(512/*rowsA*/,512/*columnsA*/,arrayA_1572864__input__0,output0__inputA__0,output1__inputA__3,output2__inputA__7,output3__inputA__2,output4__inputA__4,output5__inputA__2,output6__inputA__5,output7__inputA__5); // splitTensorA_6
		cache_inv(arrayA_1572864__input__0, 262144*sizeof(int));
		cache_wbInv(splitTensorA_6__multiply7_6__0, 131072*sizeof(char));
		sendStart(7); // Core1 > Core7: splitTensorA_6__multiply7_6__0 
		sendEnd(); // Core1 > Core7: splitTensorA_6__multiply7_6__0 
		cache_wbInv(splitTensorA_6__multiply6_6__0, 131072*sizeof(char));
		sendStart(6); // Core1 > Core6: splitTensorA_6__multiply6_6__0 
		sendEnd(); // Core1 > Core6: splitTensorA_6__multiply6_6__0 
		cache_wbInv(splitTensorA_6__multiply5_6__0, 131072*sizeof(char));
		sendStart(5); // Core1 > Core5: splitTensorA_6__multiply5_6__0 
		sendEnd(); // Core1 > Core5: splitTensorA_6__multiply5_6__0 
		cache_wbInv(splitTensorA_6__multiply4_6__0, 131072*sizeof(char));
		sendStart(4); // Core1 > Core4: splitTensorA_6__multiply4_6__0 
		sendEnd(); // Core1 > Core4: splitTensorA_6__multiply4_6__0 
		cache_wbInv(splitTensorA_6__multiply3_6__0, 131072*sizeof(char));
		sendStart(3); // Core1 > Core3: splitTensorA_6__multiply3_6__0 
		sendEnd(); // Core1 > Core3: splitTensorA_6__multiply3_6__0 
		cache_wbInv(splitTensorA_6__multiply2_6__0, 131072*sizeof(char));
		sendStart(2); // Core1 > Core2: splitTensorA_6__multiply2_6__0 
		sendEnd(); // Core1 > Core2: splitTensorA_6__multiply2_6__0 
		cache_wbInv(splitTensorA_6__multiply0_6__0, 131072*sizeof(char));
		sendStart(0); // Core1 > Core0: splitTensorA_6__multiply0_6__0 
		sendEnd(); // Core1 > Core0: splitTensorA_6__multiply0_6__0 
		receiveStart(); // Core7 > Core1: splitTensorA_7__multiply1_7__0 
		receiveEnd(7); // Core7 > Core1: splitTensorA_7__multiply1_7__0 
		cache_inv(splitTensorA_7__multiply1_7__0, 131072*sizeof(char));
		transpose(512/*rowsB*/,512/*columnsB*/,arrayB_1835008__input__0,output__input__2); // transpose_7
		cache_inv(arrayB_1835008__input__0, 262144*sizeof(int));
		receiveStart(); // Core4 > Core1: splitTensorB_0__multiply1_0__0 
		receiveEnd(4); // Core4 > Core1: splitTensorB_0__multiply1_0__0 
		cache_inv(splitTensorB_0__multiply1_0__0, 131072*sizeof(char));
		receiveStart(); // Core3 > Core1: splitTensorB_1__multiply1_1__0 
		receiveEnd(3); // Core3 > Core1: splitTensorB_1__multiply1_1__0 
		cache_inv(splitTensorB_1__multiply1_1__0, 131072*sizeof(char));
		receiveStart(); // Core6 > Core1: splitTensorB_2__multiply1_2__0 
		receiveEnd(6); // Core6 > Core1: splitTensorB_2__multiply1_2__0 
		cache_inv(splitTensorB_2__multiply1_2__0, 131072*sizeof(char));
		receiveStart(); // Core5 > Core1: splitTensorB_3__multiply1_3__0 
		receiveEnd(5); // Core5 > Core1: splitTensorB_3__multiply1_3__0 
		cache_inv(splitTensorB_3__multiply1_3__0, 131072*sizeof(char));
		receiveStart(); // Core7 > Core1: splitTensorB_4__multiply1_4__0 
		receiveEnd(7); // Core7 > Core1: splitTensorB_4__multiply1_4__0 
		cache_inv(splitTensorB_4__multiply1_4__0, 131072*sizeof(char));
		receiveStart(); // Core0 > Core1: splitTensorB_5__multiply1_5__0 
		receiveEnd(0); // Core0 > Core1: splitTensorB_5__multiply1_5__0 
		cache_inv(splitTensorB_5__multiply1_5__0, 131072*sizeof(char));
		receiveStart(); // Core2 > Core1: splitTensorB_6__multiply1_6__0 
		receiveEnd(2); // Core2 > Core1: splitTensorB_6__multiply1_6__0 
		cache_inv(splitTensorB_6__multiply1_6__0, 131072*sizeof(char));
		splitB(512/*rowsB*/,512/*columnsB*/,output__input__2,output0__inputB__4,output1__inputB__3,output2__inputB__7,output3__inputB__3,output4__inputB__5,output5__inputB__2,output6__inputB__4,output7__inputB__0); // splitTensorB_7
		cache_inv(output__input__2, 262144*sizeof(int));
		cache_wbInv(splitTensorB_7__multiply7_7__0, 131072*sizeof(char));
		sendStart(7); // Core1 > Core7: splitTensorB_7__multiply7_7__0 
		sendEnd(); // Core1 > Core7: splitTensorB_7__multiply7_7__0 
		cache_wbInv(splitTensorB_7__multiply6_7__0, 131072*sizeof(char));
		sendStart(6); // Core1 > Core6: splitTensorB_7__multiply6_7__0 
		sendEnd(); // Core1 > Core6: splitTensorB_7__multiply6_7__0 
		cache_wbInv(splitTensorB_7__multiply5_7__0, 131072*sizeof(char));
		sendStart(5); // Core1 > Core5: splitTensorB_7__multiply5_7__0 
		sendEnd(); // Core1 > Core5: splitTensorB_7__multiply5_7__0 
		cache_wbInv(splitTensorB_7__multiply4_7__0, 131072*sizeof(char));
		sendStart(4); // Core1 > Core4: splitTensorB_7__multiply4_7__0 
		sendEnd(); // Core1 > Core4: splitTensorB_7__multiply4_7__0 
		cache_wbInv(splitTensorB_7__multiply3_7__0, 131072*sizeof(char));
		sendStart(3); // Core1 > Core3: splitTensorB_7__multiply3_7__0 
		sendEnd(); // Core1 > Core3: splitTensorB_7__multiply3_7__0 
		cache_wbInv(splitTensorB_7__multiply2_7__0, 131072*sizeof(char));
		sendStart(2); // Core1 > Core2: splitTensorB_7__multiply2_7__0 
		sendEnd(); // Core1 > Core2: splitTensorB_7__multiply2_7__0 
		cache_wbInv(splitTensorB_7__multiply0_7__0, 131072*sizeof(char));
		sendStart(0); // Core1 > Core0: splitTensorB_7__multiply0_7__0 
		sendEnd(); // Core1 > Core0: splitTensorB_7__multiply0_7__0 
		receiveStart(); // Core0 > Core1: multiply0_6__sum_6__0 
		receiveEnd(0); // Core0 > Core1: multiply0_6__sum_6__0 
		cache_inv(multiply0_6__sum_6__0, 1048576*sizeof(char));
		multiply(512/*rows*/,512/*columns*/,output1__inputA__2,output1__inputB__2,output__input1__6); // multiply1_0
		cache_inv(output1__inputA__2, 32768*sizeof(int));
		cache_inv(output1__inputB__2, 32768*sizeof(int));
		cache_wbInv(multiply1_0__sum_0__0, 1048576*sizeof(char));
		sendStart(4); // Core1 > Core4: multiply1_0__sum_0__0 
		sendEnd(); // Core1 > Core4: multiply1_0__sum_0__0 
		multiply(512/*rows*/,512/*columns*/,output1__inputA__7,output1__inputB__6,output__input1__1); // multiply1_1
		cache_inv(output1__inputA__7, 32768*sizeof(int));
		cache_inv(output1__inputB__6, 32768*sizeof(int));
		cache_wbInv(multiply1_1__sum_1__0, 1048576*sizeof(char));
		sendStart(3); // Core1 > Core3: multiply1_1__sum_1__0 
		sendEnd(); // Core1 > Core3: multiply1_1__sum_1__0 
		multiply(512/*rows*/,512/*columns*/,output1__inputA__5,output1__inputB__5,output__input1__7); // multiply1_2
		cache_inv(output1__inputA__5, 32768*sizeof(int));
		cache_inv(output1__inputB__5, 32768*sizeof(int));
		cache_wbInv(multiply1_2__sum_2__0, 1048576*sizeof(char));
		sendStart(2); // Core1 > Core2: multiply1_2__sum_2__0 
		sendEnd(); // Core1 > Core2: multiply1_2__sum_2__0 
		multiply(512/*rows*/,512/*columns*/,output1__inputA__0,output1__inputB__1,output__input1__4); // multiply1_3
		cache_inv(output1__inputA__0, 32768*sizeof(int));
		cache_inv(output1__inputB__1, 32768*sizeof(int));
		cache_wbInv(multiply1_3__sum_3__0, 1048576*sizeof(char));
		sendStart(5); // Core1 > Core5: multiply1_3__sum_3__0 
		sendEnd(); // Core1 > Core5: multiply1_3__sum_3__0 
		multiply(512/*rows*/,512/*columns*/,output1__inputA__6,output1__inputB__0,output__input1__2); // multiply1_4
		cache_inv(output1__inputA__6, 32768*sizeof(int));
		cache_inv(output1__inputB__0, 32768*sizeof(int));
		cache_wbInv(multiply1_4__sum_4__0, 1048576*sizeof(char));
		sendStart(0); // Core1 > Core0: multiply1_4__sum_4__0 
		sendEnd(); // Core1 > Core0: multiply1_4__sum_4__0 
		multiply(512/*rows*/,512/*columns*/,output1__inputA__1,output1__inputB__4,output__input1__3); // multiply1_5
		cache_inv(output1__inputA__1, 32768*sizeof(int));
		cache_inv(output1__inputB__4, 32768*sizeof(int));
		cache_wbInv(multiply1_5__sum_5__0, 1048576*sizeof(char));
		sendStart(4); // Core1 > Core4: multiply1_5__sum_5__0 
		sendEnd(); // Core1 > Core4: multiply1_5__sum_5__0 
		multiply(512/*rows*/,512/*columns*/,output1__inputA__3,output1__inputB__7,output__input1__5); // multiply1_6
		cache_inv(output1__inputA__3, 32768*sizeof(int));
		cache_inv(output1__inputB__7, 32768*sizeof(int));
		multiply(512/*rows*/,512/*columns*/,output1__inputA__4,output1__inputB__3,output__input1__0); // multiply1_7
		cache_inv(output1__inputA__4, 32768*sizeof(int));
		cache_inv(output1__inputB__3, 32768*sizeof(int));
		cache_wbInv(multiply1_7__sum_7__0, 1048576*sizeof(char));
		sendStart(7); // Core1 > Core7: multiply1_7__sum_7__0 
		sendEnd(); // Core1 > Core7: multiply1_7__sum_7__0 
		receiveStart(); // Core2 > Core1: multiply2_6__sum_6__0 
		receiveEnd(2); // Core2 > Core1: multiply2_6__sum_6__0 
		cache_inv(multiply2_6__sum_6__0, 1048576*sizeof(char));
		receiveStart(); // Core3 > Core1: multiply3_6__sum_6__0 
		receiveEnd(3); // Core3 > Core1: multiply3_6__sum_6__0 
		cache_inv(multiply3_6__sum_6__0, 1048576*sizeof(char));
		receiveStart(); // Core4 > Core1: multiply4_6__sum_6__0 
		receiveEnd(4); // Core4 > Core1: multiply4_6__sum_6__0 
		cache_inv(multiply4_6__sum_6__0, 1048576*sizeof(char));
		receiveStart(); // Core5 > Core1: multiply5_6__sum_6__0 
		receiveEnd(5); // Core5 > Core1: multiply5_6__sum_6__0 
		cache_inv(multiply5_6__sum_6__0, 1048576*sizeof(char));
		receiveStart(); // Core6 > Core1: multiply6_6__sum_6__0 
		receiveEnd(6); // Core6 > Core1: multiply6_6__sum_6__0 
		cache_inv(multiply6_6__sum_6__0, 1048576*sizeof(char));
		receiveStart(); // Core7 > Core1: multiply7_6__sum_6__0 
		receiveEnd(7); // Core7 > Core1: multiply7_6__sum_6__0 
		cache_inv(multiply7_6__sum_6__0, 1048576*sizeof(char));
		sum(512/*rows*/,512/*columns*/,output__input0__4,output__input1__5,output__input2__3,output__input3__4,output__input4__3,output__input5__1,output__input6__4,output__input7__4,output__arrayC_1572864__0); // sum_6
		cache_inv(output__input0__4, 262144*sizeof(long));
		cache_inv(output__input1__5, 262144*sizeof(long));
		cache_inv(output__input2__3, 262144*sizeof(long));
		cache_inv(output__input3__4, 262144*sizeof(long));
		cache_inv(output__input4__3, 262144*sizeof(long));
		cache_inv(output__input5__1, 262144*sizeof(long));
		cache_inv(output__input6__4, 262144*sizeof(long));
		cache_inv(output__input7__4, 262144*sizeof(long));
		cache_wbInv(sum_6__implode_displayResult__0, 1048576*sizeof(char));
		sendStart(7); // Core1 > Core7: sum_6__implode_displayResult__0 
		sendEnd(); // Core1 > Core7: sum_6__implode_displayResult__0 
	}
}
