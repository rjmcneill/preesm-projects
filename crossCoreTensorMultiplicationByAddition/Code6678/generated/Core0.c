/** 
 * @file Core0.c
 * @generated by C6678CPrinter
 * @date Mon Mar 30 19:16:34 BST 2015
 */
 

#include "cores.h"
#include "utils.h"
#include "communication.h"
#include "fifo.h"
#include "cache.h"

// Core Global Declaration

// Core Global Definitions
// Won't work if the shared memory is >= 512 MB 
#pragma DATA_SECTION(SharedMem, ".mySharedMem")
char SharedMem[13344]; //  size:= 13344*char
char *const multiplyTensors_6__implode_s__0 = (char*) (SharedMem+768);  // multiplyTensors_6 > implode_sumResults_0_input size:= 256*char
char *const explode_transposeTensor_0_ou__4 = (char*) (SharedMem+2688);  // explode_transposeTensor_0_output > multiplyTensors_3 size:= 32*char
char *const explode_transposeTensor_1_ou__2 = (char*) (SharedMem+9664);  // explode_transposeTensor_1_output > multiplyTensors_10 size:= 32*char
char *const explode_generateTensors_arra__16 = (char*) (SharedMem+12352);  // explode_generateTensors_arrayB > multiplyTensors_11 size:= 32*char
char *const explode_transposeTensor_0_ou__7 = (char*) (SharedMem+6912);  // explode_transposeTensor_0_output > multiplyTensors_2 size:= 32*char
char *const multiplyTensors_7__implode_s__0 = (char*) (SharedMem+512);  // multiplyTensors_7 > implode_sumResults_0_input size:= 256*char
char *const explode_transposeTensor_1_ou__7 = (char*) (SharedMem+4096);  // explode_transposeTensor_1_output > multiplyTensors_9 size:= 32*char
char *const multiplyTensors_15__implode___0 = (char*) (SharedMem+4800);  // multiplyTensors_15 > implode_sumResults_1_input size:= 256*char
char *const explode_generateTensors_arra__10 = (char*) (SharedMem+13120);  // explode_generateTensors_arrayB > multiplyTensors_2 size:= 32*char
char *const explode_generateTensors_arra__9 = (char*) (SharedMem+4160);  // explode_generateTensors_arrayB > multiplyTensors_15 size:= 32*char
char *const multiplyTensors_8__implode_s__0 = (char*) (SharedMem+9408);  // multiplyTensors_8 > implode_sumResults_1_input size:= 256*char
char *const multiplyTensors_13__implode___0 = (char*) (SharedMem+5376);  // multiplyTensors_13 > implode_sumResults_1_input size:= 256*char
char *const explode_transposeTensor_0_ou__6 = (char*) (SharedMem+12992);  // explode_transposeTensor_0_output > multiplyTensors_1 size:= 32*char
char *const explode_generateTensors_arra__12 = (char*) (SharedMem+11968);  // explode_generateTensors_arrayB > multiplyTensors_13 size:= 32*char
char *const explode_transposeTensor_1_ou__4 = (char*) (SharedMem+11840);  // explode_transposeTensor_1_output > multiplyTensors_12 size:= 32*char
char *const explode_transposeTensor_0_ou__1 = (char*) (SharedMem+6272);  // explode_transposeTensor_0_output > multiplyTensors_4 size:= 32*char
char *const transposeTensor_1__explode_t__0 = (char*) (SharedMem+12736);  // transposeTensor_1 > explode_transposeTensor_1_output size:= 256*char
char *const explode_transposeTensor_1_ou__6 = (char*) (SharedMem+9728);  // explode_transposeTensor_1_output > multiplyTensors_15 size:= 32*char
char *const explode_generateTensors_arra__11 = (char*) (SharedMem+6976);  // explode_generateTensors_arrayB > multiplyTensors_5 size:= 32*char
char *const transposeTensor_0__explode_t__0 = (char*) (SharedMem+4288);  // transposeTensor_0 > explode_transposeTensor_0_output size:= 256*char
char *const explode_transposeTensor_0_ou__0 = (char*) (SharedMem+3648);  // explode_transposeTensor_0_output > multiplyTensors_6 size:= 32*char
char *const explode_transposeTensor_1_ou__0 = (char*) (SharedMem+1792);  // explode_transposeTensor_1_output > multiplyTensors_8 size:= 32*char
char *const explode_generateTensors_arra__8 = (char*) (SharedMem+13312);  // explode_generateTensors_arrayB > multiplyTensors_7 size:= 32*char
char *const explode_transposeTensor_1_ou__1 = (char*) (SharedMem+2816);  // explode_transposeTensor_1_output > multiplyTensors_14 size:= 32*char
char *const multiplyTensors_5__implode_s__0 = (char*) (SharedMem+0);  // multiplyTensors_5 > implode_sumResults_0_input size:= 256*char
char *const explode_generateTensors_arra__3 = (char*) (SharedMem+6848);  // explode_generateTensors_arrayB > multiplyTensors_6 size:= 32*char
char *const explode_generateTensors_arra__15 = (char*) (SharedMem+2112);  // explode_generateTensors_arrayB > multiplyTensors_10 size:= 32*char
char *const explode_generateTensors_arra__0 = (char*) (SharedMem+7040);  // explode_generateTensors_arrayB > multiplyTensors_9 size:= 32*char
char *const explode_generateTensors_arra__1 = (char*) (SharedMem+12288);  // explode_generateTensors_arrayB > multiplyTensors_0 size:= 32*char
char *const implode_displayTensor_arrayC__0 = (char*) (SharedMem+2176);  // implode_displayTensor_arrayC > displayTensor size:= 512*char
char *const explode_transposeTensor_1_ou__5 = (char*) (SharedMem+3968);  // explode_transposeTensor_1_output > multiplyTensors_11 size:= 32*char
char *const explode_generateTensors_arra__14 = (char*) (SharedMem+12032);  // explode_generateTensors_arrayA > transposeTensor_0 size:= 256*char
char *const sumResults_0__implode_displa__0 = (char*) (SharedMem+3712);  // sumResults_0 > implode_displayTensor_arrayC size:= 256*char
char *const multiplyTensors_3__implode_s__0 = (char*) (SharedMem+1536);  // multiplyTensors_3 > implode_sumResults_0_input size:= 256*char
char *const explode_generateTensors_arra__2 = (char*) (SharedMem+4224);  // explode_generateTensors_arrayB > multiplyTensors_1 size:= 32*char
char *const explode_transposeTensor_0_ou__5 = (char*) (SharedMem+13184);  // explode_transposeTensor_0_output > multiplyTensors_0 size:= 32*char
char *const explode_transposeTensor_1_ou__3 = (char*) (SharedMem+13056);  // explode_transposeTensor_1_output > multiplyTensors_13 size:= 32*char
char *const explode_transposeTensor_0_ou__3 = (char*) (SharedMem+6208);  // explode_transposeTensor_0_output > multiplyTensors_5 size:= 32*char
char *const multiplyTensors_11__implode___0 = (char*) (SharedMem+5952);  // multiplyTensors_11 > implode_sumResults_1_input size:= 256*char
char *const implode_sumResults_1_input____0 = (char*) (SharedMem+7360);  // implode_sumResults_1_input > sumResults_1 size:= 2048*char
char *const generateTensors__explode_gen__1 = (char*) (SharedMem+2880);  // generateTensors > explode_generateTensors_arrayB size:= 512*char
char *const implode_sumResults_0_input____0 = (char*) (SharedMem+9792);  // implode_sumResults_0_input > sumResults_0 size:= 2048*char
char *const explode_generateTensors_arra__4 = (char*) (SharedMem+12480);  // explode_generateTensors_arrayA > transposeTensor_1 size:= 256*char
char *const explode_generateTensors_arra__7 = (char*) (SharedMem+2752);  // explode_generateTensors_arrayB > multiplyTensors_8 size:= 32*char
char *const multiplyTensors_10__implode___0 = (char*) (SharedMem+5632);  // multiplyTensors_10 > implode_sumResults_1_input size:= 256*char
char *const explode_transposeTensor_0_ou__2 = (char*) (SharedMem+13248);  // explode_transposeTensor_0_output > multiplyTensors_7 size:= 32*char
char *const explode_generateTensors_arra__5 = (char*) (SharedMem+11904);  // explode_generateTensors_arrayB > multiplyTensors_14 size:= 32*char
char *const explode_generateTensors_arra__6 = (char*) (SharedMem+4032);  // explode_generateTensors_arrayB > multiplyTensors_12 size:= 32*char
char *const sumResults_1__implode_displa__0 = (char*) (SharedMem+3392);  // sumResults_1 > implode_displayTensor_arrayC size:= 256*char
char *const multiplyTensors_12__implode___0 = (char*) (SharedMem+5120);  // multiplyTensors_12 > implode_sumResults_1_input size:= 256*char
char *const multiplyTensors_14__implode___0 = (char*) (SharedMem+4544);  // multiplyTensors_14 > implode_sumResults_1_input size:= 256*char
char *const multiplyTensors_0__implode_s__0 = (char*) (SharedMem+1280);  // multiplyTensors_0 > implode_sumResults_0_input size:= 256*char
char *const generateTensors__displayTens__0 = (char*) (SharedMem+5056);  // generateTensors > displayTensor size:= 8*char
char *const multiplyTensors_9__implode_s__0 = (char*) (SharedMem+7104);  // multiplyTensors_9 > implode_sumResults_1_input size:= 256*char
char *const explode_generateTensors_arra__13 = (char*) (SharedMem+5888);  // explode_generateTensors_arrayB > multiplyTensors_3 size:= 32*char
char *const explode_generateTensors_arra__17 = (char*) (SharedMem+12416);  // explode_generateTensors_arrayB > multiplyTensors_4 size:= 32*char
char *const multiplyTensors_4__implode_s__0 = (char*) (SharedMem+256);  // multiplyTensors_4 > implode_sumResults_0_input size:= 256*char
char *const multiplyTensors_1__implode_s__0 = (char*) (SharedMem+1024);  // multiplyTensors_1 > implode_sumResults_0_input size:= 256*char
char *const generateTensors__explode_gen__0 = (char*) (SharedMem+6336);  // generateTensors > explode_generateTensors_arrayA size:= 512*char
char *const multiplyTensors_2__implode_s__0 = (char*) (SharedMem+1856);  // multiplyTensors_2 > implode_sumResults_0_input size:= 256*char
int *const output_32__arrayA__0 = (int*) (SharedMem+11840);  // explode_transposeTensor_1_output_output_32 > multiplyTensors_12_arrayA size:= 8*int
int *const arrayB_104__arrayB__0 = (int*) (SharedMem+11968);  // explode_generateTensors_arrayB_arrayB_104 > multiplyTensors_13_arrayB size:= 8*int
long *const arrayC__input_192__1 = (long*) (SharedMem+1536);  // multiplyTensors_3_arrayC > implode_sumResults_0_input_input_192 size:= 64*long
int *const arrayA_64__input__0 = (int*) (SharedMem+12480);  // explode_generateTensors_arrayA_arrayA_64 > transposeTensor_1_input size:= 64*int
long *const arrayC__input__1 = (long*) (SharedMem+9792);  // implode_sumResults_0_input_arrayC > sumResults_0_input size:= 512*long
int *const arrayB_0__arrayB__0 = (int*) (SharedMem+12288);  // explode_generateTensors_arrayB_arrayB_0 > multiplyTensors_0_arrayB size:= 8*int
int *const arrayB_40__arrayB__0 = (int*) (SharedMem+6976);  // explode_generateTensors_arrayB_arrayB_40 > multiplyTensors_5_arrayB size:= 8*int
long *const output__arrayC_64__0 = (long*) (SharedMem+3392);  // sumResults_1_output > implode_displayTensor_arrayC_arrayC_64 size:= 64*long
int *const arrayB_72__arrayB__0 = (int*) (SharedMem+7040);  // explode_generateTensors_arrayB_arrayB_72 > multiplyTensors_9_arrayB size:= 8*int
long *const arrayC__input_64__1 = (long*) (SharedMem+7104);  // multiplyTensors_9_arrayC > implode_sumResults_1_input_input_64 size:= 64*long
int *const arrayB_88__arrayB__0 = (int*) (SharedMem+12352);  // explode_generateTensors_arrayB_arrayB_88 > multiplyTensors_11_arrayB size:= 8*int
long *const arrayC__input_256__1 = (long*) (SharedMem+256);  // multiplyTensors_4_arrayC > implode_sumResults_0_input_input_256 size:= 64*long
int *const output_24__arrayA__0 = (int*) (SharedMem+3968);  // explode_transposeTensor_1_output_output_24 > multiplyTensors_11_arrayA size:= 8*int
long *const output__arrayC_0__0 = (long*) (SharedMem+3712);  // sumResults_0_output > implode_displayTensor_arrayC_arrayC_0 size:= 64*long
long *const arrayC__input__0 = (long*) (SharedMem+7360);  // implode_sumResults_1_input_arrayC > sumResults_1_input size:= 512*long
long *const arrayC__input_448__0 = (long*) (SharedMem+4800);  // multiplyTensors_15_arrayC > implode_sumResults_1_input_input_448 size:= 64*long
int *const output_8__arrayA__0 = (int*) (SharedMem+4096);  // explode_transposeTensor_1_output_output_8 > multiplyTensors_9_arrayA size:= 8*int
int *const output_8__arrayA__1 = (int*) (SharedMem+12992);  // explode_transposeTensor_0_output_output_8 > multiplyTensors_1_arrayA size:= 8*int
long *const arrayC__input_384__0 = (long*) (SharedMem+4544);  // multiplyTensors_14_arrayC > implode_sumResults_1_input_input_384 size:= 64*long
double *const startTime__startTime__0 = (double*) (SharedMem+5056);  // generateTensors_startTime > displayTensor_startTime size:= 1*double
int *const arrayB_96__arrayB__0 = (int*) (SharedMem+4032);  // explode_generateTensors_arrayB_arrayB_96 > multiplyTensors_12_arrayB size:= 8*int
long *const arrayC__input_320__1 = (long*) (SharedMem+0);  // multiplyTensors_5_arrayC > implode_sumResults_0_input_input_320 size:= 64*long
long *const arrayC__input_448__1 = (long*) (SharedMem+512);  // multiplyTensors_7_arrayC > implode_sumResults_0_input_input_448 size:= 64*long
int *const arrayB_32__arrayB__0 = (int*) (SharedMem+12416);  // explode_generateTensors_arrayB_arrayB_32 > multiplyTensors_4_arrayB size:= 8*int
int *const arrayB_120__arrayB__0 = (int*) (SharedMem+4160);  // explode_generateTensors_arrayB_arrayB_120 > multiplyTensors_15_arrayB size:= 8*int
int *const arrayA__input__0 = (int*) (SharedMem+6336);  // generateTensors_arrayA > explode_generateTensors_arrayA_input size:= 128*int
long *const arrayC__input_128__0 = (long*) (SharedMem+5632);  // multiplyTensors_10_arrayC > implode_sumResults_1_input_input_128 size:= 64*long
int *const arrayB_64__arrayB__0 = (int*) (SharedMem+2752);  // explode_generateTensors_arrayB_arrayB_64 > multiplyTensors_8_arrayB size:= 8*int
long *const arrayC__input_320__0 = (long*) (SharedMem+5376);  // multiplyTensors_13_arrayC > implode_sumResults_1_input_input_320 size:= 64*long
int *const output_0__arrayA__1 = (int*) (SharedMem+13184);  // explode_transposeTensor_0_output_output_0 > multiplyTensors_0_arrayA size:= 8*int
long *const arrayC__input_192__0 = (long*) (SharedMem+5952);  // multiplyTensors_11_arrayC > implode_sumResults_1_input_input_192 size:= 64*long
long *const arrayC__input_0__0 = (long*) (SharedMem+1280);  // multiplyTensors_0_arrayC > implode_sumResults_0_input_input_0 size:= 64*long
int *const arrayB__arrayB__0 = (int*) (SharedMem+2880);  // generateTensors_arrayB > explode_generateTensors_arrayB_arrayB size:= 128*int
int *const output_56__arrayA__0 = (int*) (SharedMem+13248);  // explode_transposeTensor_0_output_output_56 > multiplyTensors_7_arrayA size:= 8*int
int *const arrayB_8__arrayB__0 = (int*) (SharedMem+4224);  // explode_generateTensors_arrayB_arrayB_8 > multiplyTensors_1_arrayB size:= 8*int
int *const arrayB_80__arrayB__0 = (int*) (SharedMem+2112);  // explode_generateTensors_arrayB_arrayB_80 > multiplyTensors_10_arrayB size:= 8*int
int *const arrayA_0__input__0 = (int*) (SharedMem+12032);  // explode_generateTensors_arrayA_arrayA_0 > transposeTensor_0_input size:= 64*int
long *const arrayC__input_256__0 = (long*) (SharedMem+5120);  // multiplyTensors_12_arrayC > implode_sumResults_1_input_input_256 size:= 64*long
int *const output_16__arrayA__0 = (int*) (SharedMem+9664);  // explode_transposeTensor_1_output_output_16 > multiplyTensors_10_arrayA size:= 8*int
int *const output_48__arrayA__1 = (int*) (SharedMem+3648);  // explode_transposeTensor_0_output_output_48 > multiplyTensors_6_arrayA size:= 8*int
int *const arrayB_48__arrayB__0 = (int*) (SharedMem+6848);  // explode_generateTensors_arrayB_arrayB_48 > multiplyTensors_6_arrayB size:= 8*int
int *const output_40__arrayA__1 = (int*) (SharedMem+6208);  // explode_transposeTensor_0_output_output_40 > multiplyTensors_5_arrayA size:= 8*int
int *const output_16__arrayA__1 = (int*) (SharedMem+6912);  // explode_transposeTensor_0_output_output_16 > multiplyTensors_2_arrayA size:= 8*int
int *const arrayB_56__arrayB__0 = (int*) (SharedMem+13312);  // explode_generateTensors_arrayB_arrayB_56 > multiplyTensors_7_arrayB size:= 8*int
int *const output_40__arrayA__0 = (int*) (SharedMem+13056);  // explode_transposeTensor_1_output_output_40 > multiplyTensors_13_arrayA size:= 8*int
int *const arrayB_24__arrayB__0 = (int*) (SharedMem+5888);  // explode_generateTensors_arrayB_arrayB_24 > multiplyTensors_3_arrayB size:= 8*int
int *const output_0__arrayA__0 = (int*) (SharedMem+1792);  // explode_transposeTensor_1_output_output_0 > multiplyTensors_8_arrayA size:= 8*int
int *const output_56__arrayA__1 = (int*) (SharedMem+9728);  // explode_transposeTensor_1_output_output_56 > multiplyTensors_15_arrayA size:= 8*int
int *const output__arrayA__0 = (int*) (SharedMem+4288);  // transposeTensor_0_output > explode_transposeTensor_0_output_arrayA size:= 64*int
long *const arrayC__input_64__0 = (long*) (SharedMem+1024);  // multiplyTensors_1_arrayC > implode_sumResults_0_input_input_64 size:= 64*long
long *const arrayC__input_128__1 = (long*) (SharedMem+1856);  // multiplyTensors_2_arrayC > implode_sumResults_0_input_input_128 size:= 64*long
long *const arrayC__input_384__1 = (long*) (SharedMem+768);  // multiplyTensors_6_arrayC > implode_sumResults_0_input_input_384 size:= 64*long
int *const output__arrayA__1 = (int*) (SharedMem+12736);  // transposeTensor_1_output > explode_transposeTensor_1_output_arrayA size:= 64*int
int *const arrayB_16__arrayB__0 = (int*) (SharedMem+13120);  // explode_generateTensors_arrayB_arrayB_16 > multiplyTensors_2_arrayB size:= 8*int
long *const output__arrayC__0 = (long*) (SharedMem+2176);  // implode_displayTensor_arrayC_output > displayTensor_arrayC size:= 128*long
int *const arrayB_112__arrayB__0 = (int*) (SharedMem+11904);  // explode_generateTensors_arrayB_arrayB_112 > multiplyTensors_14_arrayB size:= 8*int
int *const output_32__arrayA__1 = (int*) (SharedMem+6272);  // explode_transposeTensor_0_output_output_32 > multiplyTensors_4_arrayA size:= 8*int
int *const output_48__arrayA__0 = (int*) (SharedMem+2816);  // explode_transposeTensor_1_output_output_48 > multiplyTensors_14_arrayA size:= 8*int
long *const arrayC__input_0__1 = (long*) (SharedMem+9408);  // multiplyTensors_8_arrayC > implode_sumResults_1_input_input_0 size:= 64*long
int *const output_24__arrayA__1 = (int*) (SharedMem+2688);  // explode_transposeTensor_0_output_output_24 > multiplyTensors_3_arrayA size:= 8*int

void core0(void){
	// Initialisation(s)
	communicationInit();

	// Begin the execution loop 
	while(1){
		busy_barrier();
		generate(8/*rowsA*/,8/*columnsA*/,2/*depthA*/,8/*rowsB*/,8/*columnsB*/,2/*depthB*/,arrayA__input__0,arrayB__arrayB__0,startTime__startTime__0); // generateTensors
		cache_wbInv(generateTensors__explode_gen__0, 512*sizeof(char));
		sendStart(7); // Core0 > Core7: generateTensors__explode_gen__0 
		sendEnd(); // Core0 > Core7: generateTensors__explode_gen__0 
		receiveStart(); // Core7 > Core0: explode_generateTensors_arra__4 
		receiveEnd(7); // Core7 > Core0: explode_generateTensors_arra__4 
		cache_inv(explode_generateTensors_arra__4, 256*sizeof(char));
		// Fork explode_generateTensors_arrayB
		{
			memcpy((void*)(arrayB_0__arrayB__0+0),(void*)( arrayB__arrayB__0+0), 8*sizeof(int));
			memcpy((void*)(arrayB_8__arrayB__0+0),(void*)( arrayB__arrayB__0+8), 8*sizeof(int));
			memcpy((void*)(arrayB_16__arrayB__0+0),(void*)( arrayB__arrayB__0+16), 8*sizeof(int));
			memcpy((void*)(arrayB_24__arrayB__0+0),(void*)( arrayB__arrayB__0+24), 8*sizeof(int));
			memcpy((void*)(arrayB_32__arrayB__0+0),(void*)( arrayB__arrayB__0+32), 8*sizeof(int));
			memcpy((void*)(arrayB_40__arrayB__0+0),(void*)( arrayB__arrayB__0+40), 8*sizeof(int));
			memcpy((void*)(arrayB_48__arrayB__0+0),(void*)( arrayB__arrayB__0+48), 8*sizeof(int));
			memcpy((void*)(arrayB_56__arrayB__0+0),(void*)( arrayB__arrayB__0+56), 8*sizeof(int));
			memcpy((void*)(arrayB_64__arrayB__0+0),(void*)( arrayB__arrayB__0+64), 8*sizeof(int));
			memcpy((void*)(arrayB_72__arrayB__0+0),(void*)( arrayB__arrayB__0+72), 8*sizeof(int));
			memcpy((void*)(arrayB_80__arrayB__0+0),(void*)( arrayB__arrayB__0+80), 8*sizeof(int));
			memcpy((void*)(arrayB_88__arrayB__0+0),(void*)( arrayB__arrayB__0+88), 8*sizeof(int));
			memcpy((void*)(arrayB_96__arrayB__0+0),(void*)( arrayB__arrayB__0+96), 8*sizeof(int));
			memcpy((void*)(arrayB_104__arrayB__0+0),(void*)( arrayB__arrayB__0+104), 8*sizeof(int));
			memcpy((void*)(arrayB_112__arrayB__0+0),(void*)( arrayB__arrayB__0+112), 8*sizeof(int));
			memcpy((void*)(arrayB_120__arrayB__0+0),(void*)( arrayB__arrayB__0+120), 8*sizeof(int));
		}
		cache_inv(arrayB__arrayB__0, 128*sizeof(int));
		cache_wbInv(explode_generateTensors_arra__9, 32*sizeof(char));
		sendStart(3); // Core0 > Core3: explode_generateTensors_arra__9 
		sendEnd(); // Core0 > Core3: explode_generateTensors_arra__9 
		cache_wbInv(explode_generateTensors_arra__5, 32*sizeof(char));
		sendStart(3); // Core0 > Core3: explode_generateTensors_arra__5 
		sendEnd(); // Core0 > Core3: explode_generateTensors_arra__5 
		cache_wbInv(explode_generateTensors_arra__12, 32*sizeof(char));
		sendStart(5); // Core0 > Core5: explode_generateTensors_arra__12 
		sendEnd(); // Core0 > Core5: explode_generateTensors_arra__12 
		cache_wbInv(explode_generateTensors_arra__6, 32*sizeof(char));
		sendStart(4); // Core0 > Core4: explode_generateTensors_arra__6 
		sendEnd(); // Core0 > Core4: explode_generateTensors_arra__6 
		cache_wbInv(explode_generateTensors_arra__16, 32*sizeof(char));
		sendStart(1); // Core0 > Core1: explode_generateTensors_arra__16 
		sendEnd(); // Core0 > Core1: explode_generateTensors_arra__16 
		cache_wbInv(explode_generateTensors_arra__15, 32*sizeof(char));
		sendStart(6); // Core0 > Core6: explode_generateTensors_arra__15 
		sendEnd(); // Core0 > Core6: explode_generateTensors_arra__15 
		cache_wbInv(explode_generateTensors_arra__0, 32*sizeof(char));
		sendStart(2); // Core0 > Core2: explode_generateTensors_arra__0 
		sendEnd(); // Core0 > Core2: explode_generateTensors_arra__0 
		cache_wbInv(explode_generateTensors_arra__7, 32*sizeof(char));
		sendStart(2); // Core0 > Core2: explode_generateTensors_arra__7 
		sendEnd(); // Core0 > Core2: explode_generateTensors_arra__7 
		cache_wbInv(explode_generateTensors_arra__8, 32*sizeof(char));
		sendStart(6); // Core0 > Core6: explode_generateTensors_arra__8 
		sendEnd(); // Core0 > Core6: explode_generateTensors_arra__8 
		cache_wbInv(explode_generateTensors_arra__3, 32*sizeof(char));
		sendStart(1); // Core0 > Core1: explode_generateTensors_arra__3 
		sendEnd(); // Core0 > Core1: explode_generateTensors_arra__3 
		cache_wbInv(explode_generateTensors_arra__11, 32*sizeof(char));
		sendStart(7); // Core0 > Core7: explode_generateTensors_arra__11 
		sendEnd(); // Core0 > Core7: explode_generateTensors_arra__11 
		cache_wbInv(explode_generateTensors_arra__17, 32*sizeof(char));
		sendStart(7); // Core0 > Core7: explode_generateTensors_arra__17 
		sendEnd(); // Core0 > Core7: explode_generateTensors_arra__17 
		cache_wbInv(explode_generateTensors_arra__13, 32*sizeof(char));
		sendStart(4); // Core0 > Core4: explode_generateTensors_arra__13 
		sendEnd(); // Core0 > Core4: explode_generateTensors_arra__13 
		cache_wbInv(explode_generateTensors_arra__10, 32*sizeof(char));
		sendStart(2); // Core0 > Core2: explode_generateTensors_arra__10 
		sendEnd(); // Core0 > Core2: explode_generateTensors_arra__10 
		cache_wbInv(explode_generateTensors_arra__2, 32*sizeof(char));
		sendStart(3); // Core0 > Core3: explode_generateTensors_arra__2 
		sendEnd(); // Core0 > Core3: explode_generateTensors_arra__2 
		cache_wbInv(explode_generateTensors_arra__1, 32*sizeof(char));
		sendStart(5); // Core0 > Core5: explode_generateTensors_arra__1 
		sendEnd(); // Core0 > Core5: explode_generateTensors_arra__1 
		transpose(8/*rowsA*/,8/*columnsA*/,2/*depthA*/,arrayA_64__input__0,output__arrayA__1); // transposeTensor_1
		cache_inv(arrayA_64__input__0, 64*sizeof(int));
		// Fork explode_transposeTensor_1_output
		{
			memcpy((void*)(output_0__arrayA__0+0),(void*)( output__arrayA__1+0), 8*sizeof(int));
			memcpy((void*)(output_8__arrayA__0+0),(void*)( output__arrayA__1+8), 8*sizeof(int));
			memcpy((void*)(output_16__arrayA__0+0),(void*)( output__arrayA__1+16), 8*sizeof(int));
			memcpy((void*)(output_24__arrayA__0+0),(void*)( output__arrayA__1+24), 8*sizeof(int));
			memcpy((void*)(output_32__arrayA__0+0),(void*)( output__arrayA__1+32), 8*sizeof(int));
			memcpy((void*)(output_40__arrayA__0+0),(void*)( output__arrayA__1+40), 8*sizeof(int));
			memcpy((void*)(output_48__arrayA__0+0),(void*)( output__arrayA__1+48), 8*sizeof(int));
			memcpy((void*)(output_56__arrayA__1+0),(void*)( output__arrayA__1+56), 8*sizeof(int));
		}
		cache_inv(output__arrayA__1, 64*sizeof(int));
		cache_wbInv(explode_transposeTensor_1_ou__6, 32*sizeof(char));
		sendStart(3); // Core0 > Core3: explode_transposeTensor_1_ou__6 
		sendEnd(); // Core0 > Core3: explode_transposeTensor_1_ou__6 
		cache_wbInv(explode_transposeTensor_1_ou__1, 32*sizeof(char));
		sendStart(3); // Core0 > Core3: explode_transposeTensor_1_ou__1 
		sendEnd(); // Core0 > Core3: explode_transposeTensor_1_ou__1 
		cache_wbInv(explode_transposeTensor_1_ou__3, 32*sizeof(char));
		sendStart(5); // Core0 > Core5: explode_transposeTensor_1_ou__3 
		sendEnd(); // Core0 > Core5: explode_transposeTensor_1_ou__3 
		cache_wbInv(explode_transposeTensor_1_ou__4, 32*sizeof(char));
		sendStart(4); // Core0 > Core4: explode_transposeTensor_1_ou__4 
		sendEnd(); // Core0 > Core4: explode_transposeTensor_1_ou__4 
		cache_wbInv(explode_transposeTensor_1_ou__5, 32*sizeof(char));
		sendStart(1); // Core0 > Core1: explode_transposeTensor_1_ou__5 
		sendEnd(); // Core0 > Core1: explode_transposeTensor_1_ou__5 
		cache_wbInv(explode_transposeTensor_1_ou__2, 32*sizeof(char));
		sendStart(6); // Core0 > Core6: explode_transposeTensor_1_ou__2 
		sendEnd(); // Core0 > Core6: explode_transposeTensor_1_ou__2 
		cache_wbInv(explode_transposeTensor_1_ou__7, 32*sizeof(char));
		sendStart(2); // Core0 > Core2: explode_transposeTensor_1_ou__7 
		sendEnd(); // Core0 > Core2: explode_transposeTensor_1_ou__7 
		cache_wbInv(explode_transposeTensor_1_ou__0, 32*sizeof(char));
		sendStart(2); // Core0 > Core2: explode_transposeTensor_1_ou__0 
		sendEnd(); // Core0 > Core2: explode_transposeTensor_1_ou__0 
		receiveStart(); // Core4 > Core0: implode_displayTensor_arrayC__0 
		receiveEnd(4); // Core4 > Core0: implode_displayTensor_arrayC__0 
		cache_inv(implode_displayTensor_arrayC__0, 512*sizeof(char));
		display(8/*rowsA*/,8/*columnsB*/,2/*depthA*/,output__arrayC__0,startTime__startTime__0); // displayTensor
		cache_inv(output__arrayC__0, 128*sizeof(long));
		cache_inv(startTime__startTime__0, 1*sizeof(double));
	}
}
