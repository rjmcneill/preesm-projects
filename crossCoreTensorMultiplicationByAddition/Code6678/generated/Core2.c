/** 
 * @file Core2.c
 * @generated by C6678CPrinter
 * @date Mon Mar 30 19:16:34 BST 2015
 */
 

#include "cores.h"
#include "utils.h"
#include "communication.h"
#include "fifo.h"
#include "cache.h"

// Core Global Declaration
extern char *const explode_generateTensors_arra__0;  // explode_generateTensors_arrayB > multiplyTensors_9 size:= 32*char defined in Core0
extern char *const explode_generateTensors_arra__7;  // explode_generateTensors_arrayB > multiplyTensors_8 size:= 32*char defined in Core0
extern char *const explode_generateTensors_arra__10;  // explode_generateTensors_arrayB > multiplyTensors_2 size:= 32*char defined in Core0
extern char *const explode_transposeTensor_0_ou__7;  // explode_transposeTensor_0_output > multiplyTensors_2 size:= 32*char defined in Core0
extern char *const explode_transposeTensor_1_ou__7;  // explode_transposeTensor_1_output > multiplyTensors_9 size:= 32*char defined in Core0
extern char *const explode_transposeTensor_1_ou__0;  // explode_transposeTensor_1_output > multiplyTensors_8 size:= 32*char defined in Core0
extern int *const output_16__arrayA__1;  // explode_transposeTensor_0_output_output_16 > multiplyTensors_2_arrayA size:= 8*int defined in Core0
extern int *const arrayB_16__arrayB__0;  // explode_generateTensors_arrayB_arrayB_16 > multiplyTensors_2_arrayB size:= 8*int defined in Core0
extern long *const arrayC__input_128__1;  // multiplyTensors_2_arrayC > implode_sumResults_0_input_input_128 size:= 64*long defined in Core0
extern char *const multiplyTensors_2__implode_s__0;  // multiplyTensors_2 > implode_sumResults_0_input size:= 256*char defined in Core0
extern int *const output_0__arrayA__0;  // explode_transposeTensor_1_output_output_0 > multiplyTensors_8_arrayA size:= 8*int defined in Core0
extern int *const arrayB_64__arrayB__0;  // explode_generateTensors_arrayB_arrayB_64 > multiplyTensors_8_arrayB size:= 8*int defined in Core0
extern long *const arrayC__input_0__1;  // multiplyTensors_8_arrayC > implode_sumResults_1_input_input_0 size:= 64*long defined in Core0
extern char *const multiplyTensors_8__implode_s__0;  // multiplyTensors_8 > implode_sumResults_1_input size:= 256*char defined in Core0
extern int *const output_8__arrayA__0;  // explode_transposeTensor_1_output_output_8 > multiplyTensors_9_arrayA size:= 8*int defined in Core0
extern int *const arrayB_72__arrayB__0;  // explode_generateTensors_arrayB_arrayB_72 > multiplyTensors_9_arrayB size:= 8*int defined in Core0
extern long *const arrayC__input_64__1;  // multiplyTensors_9_arrayC > implode_sumResults_1_input_input_64 size:= 64*long defined in Core0
extern char *const multiplyTensors_9__implode_s__0;  // multiplyTensors_9 > implode_sumResults_1_input size:= 256*char defined in Core0

// Core Global Definitions

void core2(void){
	// Initialisation(s)
	communicationInit();

	// Begin the execution loop 
	while(1){
		busy_barrier();
		receiveStart(); // Core0 > Core2: explode_generateTensors_arra__0 
		receiveEnd(0); // Core0 > Core2: explode_generateTensors_arra__0 
		cache_inv(explode_generateTensors_arra__0, 32*sizeof(char));
		receiveStart(); // Core0 > Core2: explode_generateTensors_arra__7 
		receiveEnd(0); // Core0 > Core2: explode_generateTensors_arra__7 
		cache_inv(explode_generateTensors_arra__7, 32*sizeof(char));
		receiveStart(); // Core0 > Core2: explode_generateTensors_arra__10 
		receiveEnd(0); // Core0 > Core2: explode_generateTensors_arra__10 
		cache_inv(explode_generateTensors_arra__10, 32*sizeof(char));
		receiveStart(); // Core7 > Core2: explode_transposeTensor_0_ou__7 
		receiveEnd(7); // Core7 > Core2: explode_transposeTensor_0_ou__7 
		cache_inv(explode_transposeTensor_0_ou__7, 32*sizeof(char));
		receiveStart(); // Core0 > Core2: explode_transposeTensor_1_ou__7 
		receiveEnd(0); // Core0 > Core2: explode_transposeTensor_1_ou__7 
		cache_inv(explode_transposeTensor_1_ou__7, 32*sizeof(char));
		receiveStart(); // Core0 > Core2: explode_transposeTensor_1_ou__0 
		receiveEnd(0); // Core0 > Core2: explode_transposeTensor_1_ou__0 
		cache_inv(explode_transposeTensor_1_ou__0, 32*sizeof(char));
		multiply(8/*rowsA*/,8/*columnsA*/,2/*depthA*/,8/*rowsB*/,8/*columnsB*/,2/*depthB*/,output_16__arrayA__1,arrayB_16__arrayB__0,arrayC__input_128__1); // multiplyTensors_2
		cache_inv(output_16__arrayA__1, 8*sizeof(int));
		cache_inv(arrayB_16__arrayB__0, 8*sizeof(int));
		cache_wbInv(multiplyTensors_2__implode_s__0, 256*sizeof(char));
		sendStart(7); // Core2 > Core7: multiplyTensors_2__implode_s__0 
		sendEnd(); // Core2 > Core7: multiplyTensors_2__implode_s__0 
		multiply(8/*rowsA*/,8/*columnsA*/,2/*depthA*/,8/*rowsB*/,8/*columnsB*/,2/*depthB*/,output_0__arrayA__0,arrayB_64__arrayB__0,arrayC__input_0__1); // multiplyTensors_8
		cache_inv(output_0__arrayA__0, 8*sizeof(int));
		cache_inv(arrayB_64__arrayB__0, 8*sizeof(int));
		cache_wbInv(multiplyTensors_8__implode_s__0, 256*sizeof(char));
		sendStart(4); // Core2 > Core4: multiplyTensors_8__implode_s__0 
		sendEnd(); // Core2 > Core4: multiplyTensors_8__implode_s__0 
		multiply(8/*rowsA*/,8/*columnsA*/,2/*depthA*/,8/*rowsB*/,8/*columnsB*/,2/*depthB*/,output_8__arrayA__0,arrayB_72__arrayB__0,arrayC__input_64__1); // multiplyTensors_9
		cache_inv(output_8__arrayA__0, 8*sizeof(int));
		cache_inv(arrayB_72__arrayB__0, 8*sizeof(int));
		cache_wbInv(multiplyTensors_9__implode_s__0, 256*sizeof(char));
		sendStart(4); // Core2 > Core4: multiplyTensors_9__implode_s__0 
		sendEnd(); // Core2 > Core4: multiplyTensors_9__implode_s__0 
	}
}
