/** 
 * @file Core0.c
 * @generated by C6678CPrinter
 * @date Fri Mar 13 00:17:40 GMT 2015
 */
 

#include "cores.h"
#include "utils.h"
#include "communication.h"
#include "fifo.h"
#include "cache.h"

// Core Global Declaration

// Core Global Definitions
// Won't work if the shared memory is >= 512 MB 
#pragma DATA_SECTION(SharedMem, ".mySharedMem")
char SharedMem[27520]; //  size:= 27520*char
char *const explode_generateTensors_arra__1 = (char*) (SharedMem+11520);  // explode_generateTensors_arrayA > multiplyTensors_6 size:= 512*char
char *const explode_generateTensors_arra__10 = (char*) (SharedMem+11008);  // explode_generateTensors_arrayA > multiplyTensors_5 size:= 512*char
char *const explode_generateTensors_arra__14 = (char*) (SharedMem+10496);  // explode_generateTensors_arrayA > multiplyTensors_4 size:= 512*char
char *const multiplyTensors_14__implode___0 = (char*) (SharedMem+23936);  // multiplyTensors_14 > implode_displayTensor_arrayC size:= 512*char
char *const multiplyTensors_12__implode___0 = (char*) (SharedMem+22912);  // multiplyTensors_12 > implode_displayTensor_arrayC size:= 512*char
char *const explode_generateTensors_arra__12 = (char*) (SharedMem+13568);  // explode_generateTensors_arrayA > multiplyTensors_10 size:= 512*char
char *const generateTensors__explode_gen__0 = (char*) (SharedMem+128);  // generateTensors > explode_generateTensors_arrayB size:= 8192*char
char *const broadcastTensorB_0__multiply__5 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_6 size:= 4096*char
char *const multiplyTensors_13__implode___0 = (char*) (SharedMem+25984);  // multiplyTensors_13 > implode_displayTensor_arrayC size:= 512*char
char *const broadcastTensorB_1__multiply__6 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_9 size:= 4096*char
char *const explode_generateTensors_arra__5 = (char*) (SharedMem+15616);  // explode_generateTensors_arrayA > multiplyTensors_14 size:= 512*char
char *const broadcastTensorB_0__multiply__2 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_0 size:= 4096*char
char *const explode_generateTensors_arra__3 = (char*) (SharedMem+9984);  // explode_generateTensors_arrayA > multiplyTensors_3 size:= 512*char
char *const broadcastTensorB_0__multiply__4 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_7 size:= 4096*char
char *const generateTensors__explode_gen__1 = (char*) (SharedMem+8448);  // generateTensors > explode_generateTensors_arrayA size:= 8192*char
char *const broadcastTensorB_1__multiply__1 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_15 size:= 4096*char
char *const explode_generateTensors_arra__8 = (char*) (SharedMem+4224);  // explode_generateTensors_arrayB > broadcastTensorB_1 size:= 4096*char
char *const multiplyTensors_3__implode_d__0 = (char*) (SharedMem+27008);  // multiplyTensors_3 > implode_displayTensor_arrayC size:= 512*char
char *const explode_generateTensors_arra__17 = (char*) (SharedMem+12544);  // explode_generateTensors_arrayA > multiplyTensors_8 size:= 512*char
char *const explode_generateTensors_arra__0 = (char*) (SharedMem+13056);  // explode_generateTensors_arrayA > multiplyTensors_9 size:= 512*char
char *const broadcastTensorB_0__multiply__7 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_2 size:= 4096*char
char *const multiplyTensors_7__implode_d__0 = (char*) (SharedMem+15104);  // multiplyTensors_7 > implode_displayTensor_arrayC size:= 512*char
char *const multiplyTensors_0__implode_d__0 = (char*) (SharedMem+16768);  // multiplyTensors_0 > implode_displayTensor_arrayC size:= 512*char
char *const explode_generateTensors_arra__16 = (char*) (SharedMem+15104);  // explode_generateTensors_arrayA > multiplyTensors_13 size:= 512*char
char *const broadcastTensorB_1__multiply__7 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_14 size:= 4096*char
char *const broadcastTensorB_0__multiply__1 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_4 size:= 4096*char
char *const explode_generateTensors_arra__7 = (char*) (SharedMem+12032);  // explode_generateTensors_arrayA > multiplyTensors_7 size:= 512*char
char *const broadcastTensorB_1__multiply__4 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_8 size:= 4096*char
char *const multiplyTensors_5__implode_d__0 = (char*) (SharedMem+9984);  // multiplyTensors_5 > implode_displayTensor_arrayC size:= 512*char
char *const multiplyTensors_8__implode_d__0 = (char*) (SharedMem+20864);  // multiplyTensors_8 > implode_displayTensor_arrayC size:= 512*char
char *const multiplyTensors_2__implode_d__0 = (char*) (SharedMem+17792);  // multiplyTensors_2 > implode_displayTensor_arrayC size:= 512*char
char *const broadcastTensorB_1__multiply__0 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_10 size:= 4096*char
char *const explode_generateTensors_arra__11 = (char*) (SharedMem+8448);  // explode_generateTensors_arrayA > multiplyTensors_0 size:= 512*char
char *const multiplyTensors_1__implode_d__0 = (char*) (SharedMem+26496);  // multiplyTensors_1 > implode_displayTensor_arrayC size:= 512*char
char *const broadcastTensorB_0__multiply__0 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_1 size:= 4096*char
char *const broadcastTensorB_1__multiply__5 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_12 size:= 4096*char
char *const generateTensors__displayTens__0 = (char*) (SharedMem+0);  // generateTensors > displayTensor size:= 8*char
char *const explode_generateTensors_arra__13 = (char*) (SharedMem+9472);  // explode_generateTensors_arrayA > multiplyTensors_2 size:= 512*char
char *const multiplyTensors_9__implode_d__0 = (char*) (SharedMem+13568);  // multiplyTensors_9 > implode_displayTensor_arrayC size:= 512*char
char *const explode_generateTensors_arra__9 = (char*) (SharedMem+14592);  // explode_generateTensors_arrayA > multiplyTensors_12 size:= 512*char
char *const implode_displayTensor_arrayC__0 = (char*) (SharedMem+16768);  // implode_displayTensor_arrayC > displayTensor size:= 8192*char
char *const explode_generateTensors_arra__6 = (char*) (SharedMem+128);  // explode_generateTensors_arrayB > broadcastTensorB_0 size:= 4096*char
char *const broadcastTensorB_0__multiply__6 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_3 size:= 4096*char
char *const explode_generateTensors_arra__2 = (char*) (SharedMem+8960);  // explode_generateTensors_arrayA > multiplyTensors_1 size:= 512*char
char *const explode_generateTensors_arra__15 = (char*) (SharedMem+14080);  // explode_generateTensors_arrayA > multiplyTensors_11 size:= 512*char
char *const multiplyTensors_6__implode_d__0 = (char*) (SharedMem+19840);  // multiplyTensors_6 > implode_displayTensor_arrayC size:= 512*char
char *const broadcastTensorB_0__multiply__3 = (char*) (SharedMem+128);  // broadcastTensorB_0 > multiplyTensors_5 size:= 4096*char
char *const multiplyTensors_11__implode___0 = (char*) (SharedMem+25472);  // multiplyTensors_11 > implode_displayTensor_arrayC size:= 512*char
char *const explode_generateTensors_arra__4 = (char*) (SharedMem+16128);  // explode_generateTensors_arrayA > multiplyTensors_15 size:= 512*char
char *const multiplyTensors_10__implode___0 = (char*) (SharedMem+21888);  // multiplyTensors_10 > implode_displayTensor_arrayC size:= 512*char
char *const broadcastTensorB_1__multiply__2 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_13 size:= 4096*char
char *const broadcastTensorB_1__multiply__3 = (char*) (SharedMem+4224);  // broadcastTensorB_1 > multiplyTensors_11 size:= 4096*char
char *const multiplyTensors_4__implode_d__0 = (char*) (SharedMem+18816);  // multiplyTensors_4 > implode_displayTensor_arrayC size:= 512*char
char *const multiplyTensors_15__implode___0 = (char*) (SharedMem+24960);  // multiplyTensors_15 > implode_displayTensor_arrayC size:= 512*char
int *const output_0__arrayB__1 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_0 > multiplyTensors_8_arrayB size:= 1024*int
long *const arrayC__arrayC__0 = (long*) (SharedMem+16768);  // implode_displayTensor_arrayC_arrayC > displayTensor_arrayC size:= 2048*long
int *const arrayA_1024__arrayA__0 = (int*) (SharedMem+12544);  // explode_generateTensors_arrayA_arrayA_1024 > multiplyTensors_8_arrayA size:= 128*int
long *const arrayC__arrayC_1280__0 = (long*) (SharedMem+21888);  // multiplyTensors_10_arrayC > implode_displayTensor_arrayC_arrayC_1280 size:= 128*long
long *const arrayC__arrayC_512__0 = (long*) (SharedMem+18816);  // multiplyTensors_4_arrayC > implode_displayTensor_arrayC_arrayC_512 size:= 128*long
long *const arrayC__arrayC_1152__0 = (long*) (SharedMem+13568);  // multiplyTensors_9_arrayC > implode_displayTensor_arrayC_arrayC_1152 size:= 128*long
int *const arrayA_1408__arrayA__0 = (int*) (SharedMem+14080);  // explode_generateTensors_arrayA_arrayA_1408 > multiplyTensors_11_arrayA size:= 128*int
int *const arrayA_512__arrayA__0 = (int*) (SharedMem+10496);  // explode_generateTensors_arrayA_arrayA_512 > multiplyTensors_4_arrayA size:= 128*int
int *const arrayA_128__arrayA__0 = (int*) (SharedMem+8960);  // explode_generateTensors_arrayA_arrayA_128 > multiplyTensors_1_arrayA size:= 128*int
long *const arrayC__arrayC_1408__0 = (long*) (SharedMem+25472);  // multiplyTensors_11_arrayC > implode_displayTensor_arrayC_arrayC_1408 size:= 128*long
int *const output_1024__arrayB__1 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_1024 > multiplyTensors_9_arrayB size:= 1024*int
int *const arrayA_384__arrayA__0 = (int*) (SharedMem+9984);  // explode_generateTensors_arrayA_arrayA_384 > multiplyTensors_3_arrayA size:= 128*int
int *const arrayA_1664__arrayA__0 = (int*) (SharedMem+15104);  // explode_generateTensors_arrayA_arrayA_1664 > multiplyTensors_13_arrayA size:= 128*int
int *const output_4096__arrayB__1 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_4096 > multiplyTensors_12_arrayB size:= 1024*int
int *const output_7168__arrayB__0 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_7168 > multiplyTensors_15_arrayB size:= 1024*int
int *const arrayA_896__arrayA__0 = (int*) (SharedMem+12032);  // explode_generateTensors_arrayA_arrayA_896 > multiplyTensors_7_arrayA size:= 128*int
int *const output_0__arrayB__0 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_0 > multiplyTensors_0_arrayB size:= 1024*int
int *const arrayA_256__arrayA__0 = (int*) (SharedMem+9472);  // explode_generateTensors_arrayA_arrayA_256 > multiplyTensors_2_arrayA size:= 128*int
int *const arrayA_640__arrayA__0 = (int*) (SharedMem+11008);  // explode_generateTensors_arrayA_arrayA_640 > multiplyTensors_5_arrayA size:= 128*int
long *const arrayC__arrayC_640__0 = (long*) (SharedMem+9984);  // multiplyTensors_5_arrayC > implode_displayTensor_arrayC_arrayC_640 size:= 128*long
long *const arrayC__arrayC_1536__0 = (long*) (SharedMem+22912);  // multiplyTensors_12_arrayC > implode_displayTensor_arrayC_arrayC_1536 size:= 128*long
int *const arrayB__input__0 = (int*) (SharedMem+128);  // generateTensors_arrayB > explode_generateTensors_arrayB_input size:= 2048*int
int *const arrayB_0__input__0 = (int*) (SharedMem+128);  // explode_generateTensors_arrayB_arrayB_0 > broadcastTensorB_0_input size:= 1024*int
double *const startTime__startTime__0 = (double*) (SharedMem+0);  // generateTensors_startTime > displayTensor_startTime size:= 1*double
int *const output_6144__arrayB__0 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_6144 > multiplyTensors_6_arrayB size:= 1024*int
long *const arrayC__arrayC_0__0 = (long*) (SharedMem+16768);  // multiplyTensors_0_arrayC > implode_displayTensor_arrayC_arrayC_0 size:= 128*long
int *const arrayA_768__arrayA__0 = (int*) (SharedMem+11520);  // explode_generateTensors_arrayA_arrayA_768 > multiplyTensors_6_arrayA size:= 128*int
long *const arrayC__arrayC_1920__0 = (long*) (SharedMem+24960);  // multiplyTensors_15_arrayC > implode_displayTensor_arrayC_arrayC_1920 size:= 128*long
int *const output_2048__arrayB__0 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_2048 > multiplyTensors_10_arrayB size:= 1024*int
int *const output_5120__arrayB__1 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_5120 > multiplyTensors_13_arrayB size:= 1024*int
int *const arrayA_1792__arrayA__0 = (int*) (SharedMem+15616);  // explode_generateTensors_arrayA_arrayA_1792 > multiplyTensors_14_arrayA size:= 128*int
long *const arrayC__arrayC_384__0 = (long*) (SharedMem+27008);  // multiplyTensors_3_arrayC > implode_displayTensor_arrayC_arrayC_384 size:= 128*long
long *const arrayC__arrayC_1024__0 = (long*) (SharedMem+20864);  // multiplyTensors_8_arrayC > implode_displayTensor_arrayC_arrayC_1024 size:= 128*long
long *const arrayC__arrayC_896__0 = (long*) (SharedMem+15104);  // multiplyTensors_7_arrayC > implode_displayTensor_arrayC_arrayC_896 size:= 128*long
int *const arrayA_1920__arrayA__0 = (int*) (SharedMem+16128);  // explode_generateTensors_arrayA_arrayA_1920 > multiplyTensors_15_arrayA size:= 128*int
int *const output_1024__arrayB__0 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_1024 > multiplyTensors_1_arrayB size:= 1024*int
int *const output_3072__arrayB__1 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_3072 > multiplyTensors_3_arrayB size:= 1024*int
long *const arrayC__arrayC_256__0 = (long*) (SharedMem+17792);  // multiplyTensors_2_arrayC > implode_displayTensor_arrayC_arrayC_256 size:= 128*long
long *const arrayC__arrayC_1792__0 = (long*) (SharedMem+23936);  // multiplyTensors_14_arrayC > implode_displayTensor_arrayC_arrayC_1792 size:= 128*long
int *const output_5120__arrayB__0 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_5120 > multiplyTensors_5_arrayB size:= 1024*int
int *const output_6144__arrayB__1 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_6144 > multiplyTensors_14_arrayB size:= 1024*int
int *const arrayA_1280__arrayA__0 = (int*) (SharedMem+13568);  // explode_generateTensors_arrayA_arrayA_1280 > multiplyTensors_10_arrayA size:= 128*int
long *const arrayC__arrayC_1664__0 = (long*) (SharedMem+25984);  // multiplyTensors_13_arrayC > implode_displayTensor_arrayC_arrayC_1664 size:= 128*long
int *const arrayA__arrayA__0 = (int*) (SharedMem+8448);  // generateTensors_arrayA > explode_generateTensors_arrayA_arrayA size:= 2048*int
int *const output_2048__arrayB__1 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_2048 > multiplyTensors_2_arrayB size:= 1024*int
int *const arrayA_0__arrayA__0 = (int*) (SharedMem+8448);  // explode_generateTensors_arrayA_arrayA_0 > multiplyTensors_0_arrayA size:= 128*int
int *const arrayA_1536__arrayA__0 = (int*) (SharedMem+14592);  // explode_generateTensors_arrayA_arrayA_1536 > multiplyTensors_12_arrayA size:= 128*int
int *const arrayB_1024__input__0 = (int*) (SharedMem+4224);  // explode_generateTensors_arrayB_arrayB_1024 > broadcastTensorB_1_input size:= 1024*int
int *const output_4096__arrayB__0 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_4096 > multiplyTensors_4_arrayB size:= 1024*int
long *const arrayC__arrayC_128__0 = (long*) (SharedMem+26496);  // multiplyTensors_1_arrayC > implode_displayTensor_arrayC_arrayC_128 size:= 128*long
long *const arrayC__arrayC_768__0 = (long*) (SharedMem+19840);  // multiplyTensors_6_arrayC > implode_displayTensor_arrayC_arrayC_768 size:= 128*long
int *const arrayA_1152__arrayA__0 = (int*) (SharedMem+13056);  // explode_generateTensors_arrayA_arrayA_1152 > multiplyTensors_9_arrayA size:= 128*int
int *const output_7168__arrayB__1 = (int*) (SharedMem+128);  // broadcastTensorB_0_output_7168 > multiplyTensors_7_arrayB size:= 1024*int
int *const output_3072__arrayB__0 = (int*) (SharedMem+4224);  // broadcastTensorB_1_output_3072 > multiplyTensors_11_arrayB size:= 1024*int

void core0(void){
	// Initialisation(s)
	communicationInit();

	// Begin the execution loop 
	while(1){
		busy_barrier();
		generate(32/*rowsA*/,32/*columnsA*/,2/*depthA*/,32/*rowsB*/,32/*columnsB*/,2/*depthB*/,arrayA__arrayA__0,arrayB__input__0,startTime__startTime__0); // generateTensors
		cache_wbInv(generateTensors__explode_gen__1, 8192*sizeof(char));
		sendStart(7); // Core0 > Core7: generateTensors__explode_gen__1 
		sendEnd(); // Core0 > Core7: generateTensors__explode_gen__1 
		receiveStart(); // Core7 > Core0: explode_generateTensors_arra__9 
		receiveEnd(7); // Core7 > Core0: explode_generateTensors_arra__9 
		cache_inv(explode_generateTensors_arra__9, 512*sizeof(char));
		receiveStart(); // Core7 > Core0: explode_generateTensors_arra__15 
		receiveEnd(7); // Core7 > Core0: explode_generateTensors_arra__15 
		cache_inv(explode_generateTensors_arra__15, 512*sizeof(char));
		// Fork explode_generateTensors_arrayB
		{
			cache_wb(arrayB__input__0, 2048*sizeof(int));
		}
		cache_wb(((char*)arrayB__input__0) + 0, 8192);
		cache_inv(arrayB__input__0, 2048*sizeof(int));
		cache_wbInv(explode_generateTensors_arra__6, 4096*sizeof(char));
		sendStart(4); // Core0 > Core4: explode_generateTensors_arra__6 
		sendEnd(); // Core0 > Core4: explode_generateTensors_arra__6 
		// Broadcast broadcastTensorB_1
		{
			cache_wb(arrayB_1024__input__0, 1024*sizeof(int));
		}
		cache_wb(((char*)arrayB_1024__input__0) + 0, 4096);
		cache_inv(arrayB_1024__input__0, 1024*sizeof(int));
		cache_wbInv(broadcastTensorB_1__multiply__1, 4096*sizeof(char));
		sendStart(7); // Core0 > Core7: broadcastTensorB_1__multiply__1 
		sendEnd(); // Core0 > Core7: broadcastTensorB_1__multiply__1 
		cache_wbInv(broadcastTensorB_1__multiply__7, 4096*sizeof(char));
		sendStart(5); // Core0 > Core5: broadcastTensorB_1__multiply__7 
		sendEnd(); // Core0 > Core5: broadcastTensorB_1__multiply__7 
		cache_wbInv(broadcastTensorB_1__multiply__2, 4096*sizeof(char));
		sendStart(3); // Core0 > Core3: broadcastTensorB_1__multiply__2 
		sendEnd(); // Core0 > Core3: broadcastTensorB_1__multiply__2 
		cache_wbInv(broadcastTensorB_1__multiply__0, 4096*sizeof(char));
		sendStart(2); // Core0 > Core2: broadcastTensorB_1__multiply__0 
		sendEnd(); // Core0 > Core2: broadcastTensorB_1__multiply__0 
		cache_wbInv(broadcastTensorB_1__multiply__6, 4096*sizeof(char));
		sendStart(2); // Core0 > Core2: broadcastTensorB_1__multiply__6 
		sendEnd(); // Core0 > Core2: broadcastTensorB_1__multiply__6 
		cache_wbInv(broadcastTensorB_1__multiply__4, 4096*sizeof(char));
		sendStart(7); // Core0 > Core7: broadcastTensorB_1__multiply__4 
		sendEnd(); // Core0 > Core7: broadcastTensorB_1__multiply__4 
		multiply(32/*rowsA*/,32/*columnsA*/,2/*depthA*/,32/*rowsB*/,32/*columnsB*/,2/*depthB*/,arrayA_1408__arrayA__0,output_3072__arrayB__0,arrayC__arrayC_1408__0); // multiplyTensors_11
		cache_inv(arrayA_1408__arrayA__0, 128*sizeof(int));
		cache_inv(output_3072__arrayB__0, 1024*sizeof(int));
		cache_wbInv(multiplyTensors_11__implode___0, 512*sizeof(char));
		sendStart(7); // Core0 > Core7: multiplyTensors_11__implode___0 
		sendEnd(); // Core0 > Core7: multiplyTensors_11__implode___0 
		multiply(32/*rowsA*/,32/*columnsA*/,2/*depthA*/,32/*rowsB*/,32/*columnsB*/,2/*depthB*/,arrayA_1536__arrayA__0,output_4096__arrayB__1,arrayC__arrayC_1536__0); // multiplyTensors_12
		cache_inv(arrayA_1536__arrayA__0, 128*sizeof(int));
		cache_inv(output_4096__arrayB__1, 1024*sizeof(int));
		cache_wbInv(multiplyTensors_12__implode___0, 512*sizeof(char));
		sendStart(7); // Core0 > Core7: multiplyTensors_12__implode___0 
		sendEnd(); // Core0 > Core7: multiplyTensors_12__implode___0 
		receiveStart(); // Core7 > Core0: implode_displayTensor_arrayC__0 
		receiveEnd(7); // Core7 > Core0: implode_displayTensor_arrayC__0 
		cache_inv(implode_displayTensor_arrayC__0, 8192*sizeof(char));
		display(32/*rowsA*/,32/*columnsB*/,2/*depthA*/,arrayC__arrayC__0,startTime__startTime__0); // displayTensor
		cache_inv(arrayC__arrayC__0, 2048*sizeof(long));
		cache_inv(startTime__startTime__0, 1*sizeof(double));
	}
}
